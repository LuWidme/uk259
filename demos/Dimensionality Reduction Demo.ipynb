{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LuWidme/uk259/blob/main/demos/Dimensionality%20Reduction%20Demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "\n",
    "The following is a small demonstration of PCa and some practical applications for the method"
   ],
   "id": "71b6537b8fa9f379"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n"
   ],
   "id": "c0b6119cdfc36100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizing 4D data\n",
    "### Idea 1\n",
    "\n",
    "3D Plot with 4th dimension encoded as colour / point size / symbol."
   ],
   "id": "c706bbacb3995a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#run this to enable 3d plots\n",
    "# %matplotlib widget\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "df"
   ],
   "id": "609ce4c60abb4fb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "x = df['sepal length']\n",
    "y = df['sepal width']\n",
    "z = df['petal length']\n",
    "s = df['petal width']\n",
    "c = df['target']\n",
    "colors = {'Iris-setosa':\"red\", 'Iris-versicolor':\"blue\", 'Iris-virginica':\"green\"}\n",
    "print(c.unique())\n",
    "\n",
    "ax.set_xlabel(\"sepal length\")\n",
    "ax.set_ylabel(\"sepal width\")\n",
    "ax.set_zlabel(\"petal length\")\n",
    "\n",
    "ax.scatter(x, y, z, s = df['petal width']*15 , c=c.map(colors))\n",
    "\n",
    "plt.show()"
   ],
   "id": "884589abfdef7ed2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Idea 2: Principal Component Analysis \n",
    "Principal Component Analysis (PCA) attempts to identify the principal components (called \"eigenvectors\") that best describe the variance in the data using these PCs, we can visualize points in a lower dimension (*d*) by only using a combination of the first *d* PCs."
   ],
   "id": "3a4b2825dae38e6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "print (X.shape)\n",
    "sns.pairplot(data=iris.frame, hue='target')"
   ],
   "id": "acdc58e4c2b5958e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the variance explained by each PC:",
   "id": "6ccc7d2f931645de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "pca = PCA(n_components=4)\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "# lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "# X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each component\n",
    "print(\n",
    "    \"explained variance ratio for each component: %s\"\n",
    "    % str(pca.explained_variance_ratio_)\n",
    ")\n",
    "\n",
    "print(\"\\ntotal variance explained by first 2 components:\\n%f\" %sum(pca.explained_variance_ratio_[:2]))\n"
   ],
   "id": "22060af5a454d8e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(\n",
    "        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"PCA of IRIS dataset\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2') \n",
    "\"\"\"\n",
    "plt.figure()\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(\n",
    "        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"LDA of IRIS dataset\")\n",
    "\"\"\"\n",
    "plt.show()"
   ],
   "id": "8ce98029a1d4eee8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generates the base diagram\n",
    "# PCA example\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "# There is nearly a linear shape\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n"
   ],
   "id": "d3ad59e61cddf7d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Drawing Vector arrow\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='red')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal')\n"
   ],
   "id": "363a3f9caa817c15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Removing axis:\n",
    "# When using it for recuding the dimensionality\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape) # Reduced to onl\n",
    "sns.displot(X_pca,kind=\"kde\")"
   ],
   "id": "c1d09d5a12297938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Convert to PCA and draw the original version in red\n",
    "# To get a better picture let's reverse the entire thing and plot the PCA version and the original version\n",
    "# All points get pulled to the new axis and the machine still has a good idea of where the cluser is located.\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2, color=\"red\", s =5)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.6,  color=\"dodgerblue\", s =5)\n",
    "plt.axis('equal')\n"
   ],
   "id": "67844311218ed695"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
